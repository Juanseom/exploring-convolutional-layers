{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Convolutional Layers: Fashion-MNIST\n",
    "\n",
    "## Description\n",
    "This notebook explores the design and behavior of convolutional neural networks (CNNs) through systematic experimentation on the Fashion-MNIST dataset.\n",
    "\n",
    "We will analyze how architectural choices impact performance and learning.\n",
    "\n",
    "## Objectives\n",
    "1. **Dataset Exploration (EDA)**: Understand the structure, distribution, and characteristics of Fashion-MNIST\n",
    "2. **Baseline Model**: Implement a fully connected network as a performance reference\n",
    "3. **CNN Architecture Design**: Build a convolutional network with justified design decisions\n",
    "4. **Controlled Experiments**: Systematically vary one architectural aspect (kernel size) and measure its impact\n",
    "5. **Interpretation**: Explain why convolutions introduce useful inductive bias for image data\n",
    "\n",
    "## Context\n",
    "In this course, neural networks are not treated as black boxes but as **architectural components** whose design choices affect performance, scalability, and interpretability.\n",
    "We focus on **convolutional layers** as a concrete example of how **inductive bias** is introduced into learning systems.\n",
    "\n",
    "**Inductive bias:** means the assumptions a model makes about the data. For images, convolutions assume that:\n",
    "- Nearby pixels are related\n",
    "- The same pattern can appear anywhere in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection: Fashion-MNIST\n",
    "\n",
    "### Why Fashion-MNIST?\n",
    "\n",
    "We chose Fashion-MNIST from TensorFlow Keras Datasets. It is a collection of grayscale images of clothing items.\n",
    "\n",
    "**Why is this dataset appropriate for convolutional layers?**\n",
    "\n",
    "1. **Image-based data**: Each sample is a 28×28 grayscale image, which is the type of data convolutions are designed for.\n",
    "2. **Multiple classes**: It has 10 different classes (T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Bag, Sneaker, Ankle boot).\n",
    "3. **Fits in memory**: The full dataset (60,000 training + 10,000 test images) is small enough to run on any laptop.\n",
    "4. **Spatial patterns matter**: Clothing items have shapes and textures that convolutional layers can detect using local filters (edges, curves, patterns).\n",
    "5. **More challenging than MNIST digits**: Fashion items have more visual complexity than handwritten digits, so it is a better test for our CNN experiments.\n",
    "\n",
    "### Class Labels\n",
    "| Label | Class Name |\n",
    "|-------|------------|\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Bag |\n",
    "| 8 | Sneaker |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\juans\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (25.3)\n",
      "Collecting pip\n",
      "  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 24.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 14.5 MB/s  0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.3\n",
      "    Uninstalling pip-25.3:\n",
      "      Successfully uninstalled pip-25.3\n",
      "Successfully installed pip-26.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.14.exe and pip3.exe are installed in 'c:\\Users\\juans\\AppData\\Local\\Programs\\Python\\Python314\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\Users\\juans\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in c:\\Users\\juans\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\Users\\juans\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages (3.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    }
   ],
   "source": [
    "#Install required libraries\n",
    "%pip install numpy pandas matplotlib tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Exploration (EDA)\n",
    "\n",
    "The goal of this section is to understand the structure of Fashion-MNIST before building any model.\n",
    "\n",
    "We will look at:\n",
    "- Dataset size (how many images for training and testing)\n",
    "- Image dimensions and channels\n",
    "- Class distribution (are all classes balanced?)\n",
    "- Visual examples of each class\n",
    "- What preprocessing is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Dataset\n",
    "We load Fashion-MNIST directly from TensorFlow.\n",
    "\n",
    "The data comes already split into:\n",
    "- **Training set**: used to train the model\n",
    "- **Test set**: used to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Bag', 'Sneaker', 'Ankle boot']\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Size and Image Dimensions\n",
    "\n",
    "Let's check how many images we have and what they look like in terms of shape.\n",
    "\n",
    "- **Shape** tells us the dimensions: `(number_of_images, height, width)`\n",
    "- Since these are grayscale images, there is only **1 channel** (no color). Color images (like CIFAR-10) have 3 channels: Red, Green, Blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATASET SIZE\")\n",
    "print(f\"Training images: {x_train.shape[0]}\")\n",
    "print(f\"Test images:     {x_test.shape[0]}\")\n",
    "print(f\"Total images:    {x_train.shape[0] + x_test.shape[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"IMAGE DIMENSIONS\")\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape:     {x_test.shape}\")\n",
    "print(f\"Single image shape:  {x_train[0].shape}\")\n",
    "print(f\"Image height:  {x_train.shape[1]} pixels\")\n",
    "print(f\"Image width:   {x_train.shape[2]} pixels\")\n",
    "print(f\"Channels:      1 (grayscale)\")\n",
    "print()\n",
    "\n",
    "print(\"PIXEL VALUES\")\n",
    "print(f\"Data type:   {x_train.dtype}\")\n",
    "print(f\"Min value:   {x_train.min()}\")\n",
    "print(f\"Max value:   {x_train.max()}\")\n",
    "print()\n",
    "\n",
    "print(\"LABELS\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Number of classes:     {len(np.unique(y_train))}\")\n",
    "print(f\"Class labels:          {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Class Distribution\n",
    "\n",
    "It is important to check if all classes have a similar number of samples. If one class has many more images than another, the model might become **biased** towards the bigger class.\n",
    "\n",
    "A **balanced dataset** means each class has roughly the same number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes, train_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Label': unique_classes,\n",
    "    'Class Name': [class_names[i] for i in unique_classes],\n",
    "    'Train Samples': train_counts\n",
    "})\n",
    "\n",
    "_, test_counts = np.unique(y_test, return_counts=True)\n",
    "distribution_df['Test Samples'] = test_counts\n",
    "\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(distribution_df.to_string(index=False))\n",
    "print()\n",
    "print(f\"Training set - Min samples: {train_counts.min()}, Max samples: {train_counts.max()}\")\n",
    "print(f\"The dataset is balanced: each class has exactly {train_counts[0]} training samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set distribution\n",
    "axes[0].bar(range(10), train_counts, color='steelblue')\n",
    "axes[0].set_xticks(range(10))\n",
    "axes[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[0].set_title('Training Set - Samples per Class')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_xlabel('Class')\n",
    "\n",
    "# Test set distribution\n",
    "axes[1].bar(range(10), test_counts, color='coral')\n",
    "axes[1].set_xticks(range(10))\n",
    "axes[1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "axes[1].set_title('Test Set - Samples per Class')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_xlabel('Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visual Examples of Each Class\n",
    "\n",
    "We will show 3 random examples for each of the 10 classes so we can see what the model needs to learn to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 3\n",
    "fig, axes = plt.subplots(10, num_examples, figsize=(4, 13))\n",
    "\n",
    "for class_idx in range(10):\n",
    "    class_images = x_train[y_train == class_idx]\n",
    "    \n",
    "    random_indices = np.random.choice(len(class_images), num_examples, replace=False) # Random examples\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        axes[class_idx, i].imshow(class_images[idx], cmap='gray')\n",
    "        axes[class_idx, i].axis('off')\n",
    "        \n",
    "        if i == 0:\n",
    "            axes[class_idx, i].set_title(class_names[class_idx], fontsize=8, loc='left')\n",
    "\n",
    "plt.suptitle('3 Random Samples per Class', fontsize=12, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 A Closer Look at a Single Image\n",
    "\n",
    "Now we look at one image in detail to understand what the data looks like at the pixel level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = x_train[0]\n",
    "sample_label = y_train[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].imshow(sample_image, cmap='gray')\n",
    "axes[0].set_title(f'Class: {class_names[sample_label]} (label={sample_label})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "im = axes[1].imshow(sample_image, cmap='hot')\n",
    "axes[1].set_title('Pixel Values (heatmap)')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {sample_image.shape}\")\n",
    "print(f\"Pixel value range: [{sample_image.min()}, {sample_image.max()}]\")\n",
    "print(f\"Mean pixel value: {sample_image.mean():.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Preprocessing\n",
    "\n",
    "Before we can feed the data into a neural network, we need to do some preprocessing:\n",
    "\n",
    "#### Normalization\n",
    "The pixel values are currently integers from 0 to 255. Neural networks train better when input values are small, usually in the range **0 to 1**. To do this, we simply divide by 255:\n",
    "\n",
    "```\n",
    "x_train = x_train / 255.0\n",
    "```\n",
    "\n",
    "This maps:\n",
    "- 0 → 0.0\n",
    "- 255 → 1.0\n",
    "- 128 → ~0.50\n",
    "\n",
    "#### Reshape for CNN\n",
    "Convolutional layers in TensorFlow expect the input to have a channel dimension. Since our images are grayscale (1 channel), we need to reshape from `(28, 28)` to `(28, 28, 1)`.\n",
    "\n",
    "#### One-Hot Encoding of Labels\n",
    "The labels are integers from 0 to 9. For training with `categorical_crossentropy` loss, we convert them to one-hot vector:\n",
    "- Label `3` becomes `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "print(\"After normalization:\")\n",
    "print(f\"  x_train range: [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"  x_test range:  [{x_test.min()}, {x_test.max()}]\")\n",
    "print()\n",
    "\n",
    "x_train_cnn = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test_cnn = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(\"After reshape for CNN:\")\n",
    "print(f\"  x_train_cnn shape: {x_train_cnn.shape}\")\n",
    "print(f\"  x_test_cnn shape:  {x_test_cnn.shape}\")\n",
    "print()\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "y_train_oh = to_categorical(y_train, 10)\n",
    "y_test_oh = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"After one-hot encoding:\")\n",
    "print(f\"  y_train_oh shape: {y_train_oh.shape}\")\n",
    "print(f\"  Example - label {y_train[0]} becomes: {y_train_oh[0]}\")\n",
    "print()\n",
    "print(\"Preprocessing complete! Data is ready for model training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
